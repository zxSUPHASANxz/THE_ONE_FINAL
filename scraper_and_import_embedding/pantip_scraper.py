#!/usr/bin/env python3
"""
Pantip Motorcycle Scraper - Auto Search & Extract
‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Pantip ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
"""

import json
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


def create_chrome_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome WebDriver ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver


def clean_text(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def search_pantip(driver, keywords, max_retries=3):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡πÉ‡∏ô Pantip ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"""
    logger.info("üîç %s", keywords)
    
    search_url = f"https://pantip.com/search?q={keywords}"
    
    # Retry logic with exponential backoff
    for attempt in range(max_retries):
        try:
            driver.get(search_url)
            time.sleep(1.5)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 3 ‡πÄ‡∏õ‡πá‡∏ô 1.5
            break
        except Exception as e:
            error_msg = str(e).lower()
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô session error ‡πÉ‡∏´‡πâ raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà
            if 'session' in error_msg or 'invalid' in error_msg or 'disconnected' in error_msg:
                logger.error("‚ùå Session error: %s", str(e)[:200])
                raise Exception(f"RECREATE_DRIVER_NEEDED: {e}")

            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                logger.warning("Retry %d...", attempt + 1)
                time.sleep(wait_time)
            else:
                raise e
    
    # Scroll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ - ‡∏•‡∏î‡∏à‡∏≤‡∏Å 3 ‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏£‡∏≠‡∏ö
    for _ in range(2):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 2 ‡πÄ‡∏õ‡πá‡∏ô 1
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # ‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    thread_links = []
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å div.post-item
    posts = soup.find_all('div', class_='post-item')
    for post in posts:
        link = post.find('a', href=True)
        if link and '/topic/' in link.get('href'):
            full_url = f"https://pantip.com{link.get('href')}" if link.get('href').startswith('/') else link.get('href')
            if full_url not in thread_links:
                thread_links.append(full_url)
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å a tag ‡∏ó‡∏µ‡πà‡∏°‡∏µ /topic/
    if len(thread_links) == 0:
        links = soup.find_all('a', href=re.compile(r'/topic/\d+'))
        for link in links:
            full_url = f"https://pantip.com{link.get('href')}" if link.get('href').startswith('/') else link.get('href')
            # ‡∏ï‡∏±‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏≠‡∏Å
            full_url = full_url.split('?')[0]
            if full_url not in thread_links:
                thread_links.append(full_url)
    
    logger.info("‚úÖ ‡∏û‡∏ö %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ", len(thread_links))
    return thread_links


def scrape_thread_content(driver, url, max_retries=2):
    """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ Pantip"""
    for attempt in range(max_retries):
        try:
            logger.info("  üåê ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: %s", url)
            driver.get(url)
            
            # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ WebDriverWait - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                )
            except:
                pass
            
            # ‡∏£‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö lazy loading
            time.sleep(5)
            
            # Scroll ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î comments ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            last_height = driver.execute_script("return document.body.scrollHeight")
            for i in range(5):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
            
            # Scroll ‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            thread_data = {
                'url': url,
                'title': '',
                'author': '',
                'date': '',
                'views': 0,
                'comments_count': 0,
                'tags': [],
                'content': '',
                'comments': []
            }
            
            # Title - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            title_tag = soup.find('h1', class_='display-post-title')
            if not title_tag:
                title_tag = soup.find('h1', class_=re.compile(r'title'))
            if not title_tag:
                title_tag = soup.find('h1')
            if title_tag:
                thread_data['title'] = clean_text(title_tag.get_text())
            
            # Author - ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            author_tag = soup.find('a', class_='owner')
            if not author_tag:
                author_tag = soup.find('div', class_='display-post-name')
            if not author_tag:
                author_tag = soup.find('a', class_=re.compile(r'author'))
            if author_tag:
                thread_data['author'] = clean_text(author_tag.get_text())
            
            # Date
            date_tag = soup.find('abbr', class_='timeago')
            if not date_tag:
                date_tag = soup.find('time')
            if date_tag:
                thread_data['date'] = date_tag.get('title', '') or date_tag.get('datetime', '')
            
            # Views
            views_tag = soup.find('span', class_='view-count')
            if views_tag:
                views_text = clean_text(views_tag.get_text())
                views_match = re.search(r'([\d,]+)', views_text)
                if views_match:
                    thread_data['views'] = int(views_match.group(1).replace(',', ''))
            
            # Comments count
            comment_count_tag = soup.find('span', class_='comments-count')
            if comment_count_tag:
                count_text = clean_text(comment_count_tag.get_text())
                count_match = re.search(r'(\d+)', count_text)
                if count_match:
                    thread_data['comments_count'] = int(count_match.group(1))
            
            # Tags
            tag_links = soup.find_all('a', class_='tag-item')
            if not tag_links:
                tag_links = soup.find_all('a', href=re.compile(r'/tag/'))
            thread_data['tags'] = [clean_text(tag.get_text()) for tag in tag_links if tag.get_text().strip()]
            
            # ===== ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å =====
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            content_div = soup.find('div', class_='display-post-story')
            if not content_div:
                content_div = soup.find('div', class_=re.compile(r'post-story'))
            if not content_div:
                content_div = soup.find('div', attrs={'data-type': 'message'})
            if not content_div:
                # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å article ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏≤ div ‡πÅ‡∏£‡∏Å
                article = soup.find('article')
                if article:
                    content_div = article.find('div', class_=re.compile(r'(content|story|message|body)'))
            
            if content_div:
                # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                for unwanted in content_div.find_all(['script', 'style', 'iframe']):
                    unwanted.decompose()
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° - ‡πÉ‡∏ä‡πâ get_text() ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
                content_text = content_div.get_text(separator='\n', strip=True)
                thread_data['content'] = clean_text(content_text)
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å paragraph ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            if not thread_data['content'] or len(thread_data['content']) < 20:
                # ‡∏´‡∏≤‡∏ó‡∏∏‡∏Å p tag ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô article ‡∏´‡∏£‡∏∑‡∏≠ main content
                main_content = soup.find('article') or soup.find('main') or soup
                paragraphs = []
                for p in main_content.find_all(['p', 'div'], class_=re.compile(r'(paragraph|text-display)')):
                    text = clean_text(p.get_text())
                    if text and len(text) > 15 and 'comment' not in p.get('class', []):
                        paragraphs.append(text)
                
                if paragraphs:
                    thread_data['content'] = '\n\n'.join(paragraphs[:10])  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 paragraphs ‡πÅ‡∏£‡∏Å
            
            # ===== ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå =====
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            comment_divs = soup.find_all('div', class_='comment-item')
            if not comment_divs:
                comment_divs = soup.find_all('div', class_=re.compile(r'comment-wrapper'))
            if not comment_divs:
                comment_divs = soup.find_all('article', class_=re.compile(r'comment'))
            if not comment_divs:
                # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å div ‡∏ó‡∏µ‡πà‡∏°‡∏µ id ‡∏´‡∏£‡∏∑‡∏≠ class ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö comment
                comment_section = soup.find('div', id=re.compile(r'comment', re.I))
                if comment_section:
                    comment_divs = comment_section.find_all('div', recursive=True)
            
            comment_count = 0
            for comment_div in comment_divs:
                if comment_count >= 20:  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏õ‡πá‡∏ô 20 comments
                    break
                
                # ‡∏´‡∏≤ author
                comment_author_tag = comment_div.find('a', class_=re.compile(r'(author|name|owner)'))
                if not comment_author_tag:
                    comment_author_tag = comment_div.find('span', class_=re.compile(r'(author|name|user)'))
                if not comment_author_tag:
                    comment_author_tag = comment_div.find('div', class_=re.compile(r'name'))
                
                # ‡∏´‡∏≤ content
                comment_text_tag = comment_div.find('div', class_=re.compile(r'(story|content|text|message|body)'))
                if not comment_text_tag:
                    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å p tags
                    comment_paragraphs = comment_div.find_all('p')
                    if comment_paragraphs:
                        comment_text_tag = comment_paragraphs[0].parent
                
                if comment_text_tag:
                    # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                    for unwanted in comment_text_tag.find_all(['script', 'style', 'iframe']):
                        unwanted.decompose()
                    
                    comment_text = clean_text(comment_text_tag.get_text(separator='\n', strip=True))
                    
                    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏¢‡∏≤‡∏ß‡∏û‡∏≠ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà metadata
                    if len(comment_text) > 20 and not re.match(r'^(Like|Reply|Report|Share|\d+)', comment_text):
                        comment_count += 1
                        thread_data['comments'].append({
                            'author': clean_text(comment_author_tag.get_text()) if comment_author_tag else 'Anonymous',
                            'content': comment_text
                        })
            
            return thread_data
            
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 2 ‡πÄ‡∏õ‡πá‡∏ô 1
                continue
            else:
                return None
    
    return None


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    
    # ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 110+ ‡∏Ñ‡∏≥
    SEARCH_KEYWORDS = [
        # ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
        "bigbike ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "bigbike ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
        "bigbike ‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà",
        "bigbike ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ñ‡∏±‡∏ô‡πÅ‡∏£‡∏Å",
        "bigbike ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å",
        
        # ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 250cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 300cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 400cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 500cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 650cc",
        
        # Kawasaki
        "ninja 250",
        "ninja 300",
        "ninja 400",
        "ninja 650",
        "ninja 1000",
        "z250",
        "z300",
        "z400",
        "z650",
        "z900",
        "versys 650",
        "versys 1000",
        "er6n",
        "er6f",
        
        # Honda
        "cbr150",
        "cbr250",
        "cbr300",
        "cbr500",
        "cbr650",
        "cbr1000",
        "cb150r",
        "cb300f",
        "cb500x",
        "cb650r",
        "rebel 300",
        "rebel 500",
        "pcx 150",
        "pcx 160",
        "forza 300",
        
        # Yamaha
        "r15",
        "r3",
        "r6",
        "r1",
        "mt-03",
        "mt-07",
        "mt-09",
        "mt-10",
        "xsr700",
        "xsr900",
        "tracer 700",
        "tracer 900",
        "aerox 155",
        "nmax 155",
        
        # Suzuki
        "gsx-r150",
        "gsx-s150",
        "gsx250r",
        "gsx-s750",
        "gsx-s1000",
        "sv650",
        "v-strom 650",
        "hayabusa",
        
        # KTM
        "duke 200",
        "duke 250",
        "duke 390",
        "duke 790",
        "rc 200",
        "rc 390",
        "adventure 390",
        
        # BMW
        "bmw s1000rr",
        "bmw f750gs",
        "bmw f850gs",
        "bmw r1250gs",
        
        # Ducati
        "ducati monster",
        "ducati panigale",
        "ducati scrambler",
        
        # Triumph
        "triumph street triple",
        "triumph speed triple",
        "triumph tiger",
        
        # Harley Davidson
        "harley davidson",
        "harley sportster",
        
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏ã‡πà‡∏≠‡∏°‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≠‡πÑ‡∏ã",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏°‡∏≠‡πÑ‡∏ã",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤ ===
        "‡∏ã‡πà‡∏≠‡∏°‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ñ‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏•‡πâ‡∏≤‡∏á‡πÇ‡∏ã‡πà‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏î‡∏π‡πÅ‡∏•‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà/‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå ===
        "‡∏¢‡∏≤‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÑ‡∏ü‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ó‡πà‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ú‡πâ‡∏≤‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÇ‡∏ä‡πâ‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏Ñ‡∏•‡∏±‡∏ï‡∏ä‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤/‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏î‡∏±‡∏ö",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏£‡πâ‡∏≠‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏™‡∏±‡πà‡∏ô",
        "‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏¢‡∏≤‡∏Å",
        
        # === ‡∏£‡∏∏‡πà‡∏ô‡∏£‡∏ñ‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ===
        "xmax 300",
        "tmax 560",
        "msx 125",
        "monkey 125",
        "grom",
        "benelli tnt 150",
        "benelli 302r",
        "cfmoto 300nk",
        "gpx gentleman 200",
        "gpx legend 250",
        
        # === ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ó‡∏±‡∏ß‡∏£‡πå‡∏£‡∏¥‡πà‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÑ‡∏Å‡∏•‡πÅ‡∏î‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡∏≠‡∏¢",
        "‡∏Ç‡∏±‡∏ö‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ñ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ñ‡∏ô‡πÄ‡∏ï‡∏µ‡πâ‡∏¢",
        
        # === ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢/‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô ===
        "‡∏Ñ‡πà‡∏≤‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏†‡∏≤‡∏©‡∏µ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ú‡πà‡∏≠‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á",
        
        # === ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå Safety ===
        "‡∏´‡∏°‡∏ß‡∏Å‡∏Å‡∏±‡∏ô‡∏ô‡πá‡∏≠‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ñ‡∏∏‡∏á‡∏°‡∏∑‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏ö‡∏π‡πä‡∏ó‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "ABS ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏£‡∏±‡πà‡∏ß",
        "‡∏¢‡∏≤‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÅ‡∏ï‡∏Å",
        "‡∏Ñ‡∏•‡∏±‡∏ï‡∏ä‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏•‡∏∑‡πà‡∏ô",
        "‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏≠‡πà‡∏≠‡∏ô",
        "‡πÑ‡∏ü‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î",
        "‡πÅ‡∏ö‡∏ï‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏´‡∏°‡∏î",
    ]
    
    OUTPUT_FILE = "pantip.json"
    MAX_THREADS_PER_KEYWORD = 100  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 15 ‡πÄ‡∏õ‡πá‡∏ô 10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    RECREATE_DRIVER_EVERY = 10  # ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å 10 ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô (‡∏•‡∏î‡∏à‡∏≤‡∏Å 20)
    AUTO_SAVE_EVERY = 5  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å 5 keywords
    
    logger.info("%s", "=" * 80)
    logger.info("üöÄ Pantip Motorcycle Scraper - FAST MODE")
    logger.info("%s", "=" * 80)
    logger.info("üìã ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: %d ‡∏Ñ‡∏≥", len(SEARCH_KEYWORDS))
    logger.info("üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå")
    logger.info("üìÅ Output: %s", OUTPUT_FILE)
    logger.info("üî¢ ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ/‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô", MAX_THREADS_PER_KEYWORD)
    logger.info("%s", "=" * 80 + "\n")
    
    driver = create_chrome_driver()
    
    all_threads = []
    seen_urls = set()
    start_time = datetime.now()
    
    try:
        for idx, keyword in enumerate(SEARCH_KEYWORDS):
            # Progress percentage
            progress = (idx / len(SEARCH_KEYWORDS)) * 100
            
            # ETA calculation
            if idx > 0:
                elapsed = (datetime.now() - start_time).total_seconds()
                avg_time = elapsed / idx
                remaining = len(SEARCH_KEYWORDS) - idx
                eta_seconds = avg_time * remaining
                eta_str = f"{int(eta_seconds//60)}m {int(eta_seconds%60)}s"
            else:
                eta_str = "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì..."
            
            logger.info("\n[%5.1f%%] [%d/%d] ETA: %s | ", progress, idx+1, len(SEARCH_KEYWORDS), eta_str)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å N keywords
            if idx > 0 and idx % RECREATE_DRIVER_EVERY == 0:
                logger.info("\nüîÑ Driver refresh...")
                try:
                    driver.quit()
                except:
                    pass
                time.sleep(2)
                driver = create_chrome_driver()
                logger.info("OK")
                time.sleep(2)
            
            # Auto-save ‡∏ó‡∏∏‡∏Å N keywords
            if idx > 0 and idx % AUTO_SAVE_EVERY == 0 and all_threads:
                logger.info("\nüíæ Auto-save... (%d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ)", len(all_threads))
                try:
                    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                        json.dump(all_threads, f, ensure_ascii=False, indent=2)
                    logger.info("‚úÖ")
                except Exception as e:
                    logger.error("‚ùå %s", e)
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ
            try:
                thread_urls = search_pantip(driver, keyword)
            except Exception as e:
                error_msg = str(e)
                logger.error("‚ùå Error: %s", error_msg[:200])
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô session error ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ RECREATE_DRIVER_NEEDED
                if 'RECREATE_DRIVER_NEEDED' in error_msg or 'session' in error_msg.lower() or 'invalid' in error_msg.lower() or 'disconnected' in error_msg.lower():
                    logger.info("üîÑ Recreating driver due to session error...")
                    try:
                        driver.quit()
                    except:
                        pass
                    time.sleep(3)
                    driver = create_chrome_driver()
                    logger.info("‚úÖ Driver recreated")
                    
                    # ‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà
                    try:
                        thread_urls = search_pantip(driver, keyword)
                    except Exception as retry_error:
                        logger.error("‚ùå Retry failed: %s", str(retry_error)[:200])
                        continue
                else:
                    continue
                time.sleep(2)
                try:
                    thread_urls = search_pantip(driver, keyword)
                except:
                    logger.warning("Skip after retry")
                    continue
            
            logger.info("‚Üí %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ", len(thread_urls))
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
            thread_urls = thread_urls[:MAX_THREADS_PER_KEYWORD]
            
            # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ
            total_threads = len(thread_urls)
            
            for i, url in enumerate(thread_urls, 1):
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ã‡πâ‡∏≥
                if url in seen_urls:
                    continue
                
                seen_urls.add(url)
                logger.info("  [%d/%d]", i, len(thread_urls))
                
                # ‡∏•‡∏≠‡∏á‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏ñ‡πâ‡∏≤ error ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà
                thread_data = None
                try:
                    thread_data = scrape_thread_content(driver, url)
                except Exception as e:
                    logger.exception("  ‚ùå Error: %s", e)
                    logger.info("  üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà...")
                    try:
                        driver.quit()
                    except:
                        pass
                    driver = create_chrome_driver()
                    time.sleep(2)
                    # ‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                    try:
                        thread_data = scrape_thread_content(driver, url)
                    except:
                        logger.info("  ‚è≠Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ô‡∏µ‡πâ")
                
                if thread_data and thread_data.get('content'):
                    all_threads.append(thread_data)
                    logger.info("‚úÖ %s... (%d chars)", thread_data['title'][:50], len(thread_data['content']))
                else:
                    logger.info("‚è≠Ô∏è Skip (no content)")
                
                # ‡∏û‡∏±‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                time.sleep(1)  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 2 ‡πÄ‡∏õ‡πá‡∏ô 1
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ
            logger.info("  ‚Üí ‡∏£‡∏ß‡∏°: %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ | ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: %d URLs", len(all_threads), len(seen_urls))
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        logger.info("\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_threads, f, ensure_ascii=False, indent=2)
        
        logger.info("\n%s", "=" * 80)
        logger.info("‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå: %s", OUTPUT_FILE)
        logger.info("üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: %d", len(all_threads))
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        import os
        file_size = os.path.getsize(OUTPUT_FILE)
        logger.info("üìÅ ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå: %d bytes (%.2f KB)", file_size, file_size/1024)
        
        total_content = sum(len(t['content']) for t in all_threads)
        total_comments = sum(len(t['comments']) for t in all_threads)
        total_views = sum(t['views'] for t in all_threads)
        
        logger.info("\nüìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:")
        logger.info("  - ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏£‡∏ß‡∏°: %d ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£", total_content)
        logger.info("  - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏ß‡∏°: %d comments", total_comments)
        logger.info("  - ‡∏¢‡∏≠‡∏î‡∏ß‡∏¥‡∏ß‡∏£‡∏ß‡∏°: %d ‡∏Ñ‡∏£‡∏±‡πâ‡∏á", total_views)
        logger.info("  - ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: %d ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£", (total_content//len(all_threads) if all_threads else 0))
        logger.info("  - comments ‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: %.1f", (total_comments//len(all_threads) if all_threads else 0))
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏û‡∏ö
        brands_found = {}
        brand_keywords = ['honda', 'yamaha', 'kawasaki', 'suzuki', 'ktm', 'ducati', 'bmw', 'triumph', 'harley']
        for thread in all_threads:
            title_lower = thread['title'].lower()
            for brand in brand_keywords:
                if brand in title_lower:
                    brands_found[brand] = brands_found.get(brand, 0) + 1
        
        if brands_found:
            logger.info("\nüèçÔ∏è  ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:")
            for brand, count in sorted(brands_found.items(), key=lambda x: x[1], reverse=True)[:5]:
                logger.info("  - %s: %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ", brand.upper(), count)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        logger.info("\nüìã ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ (‡πÅ‡∏™‡∏î‡∏á 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å):")
        for i, thread in enumerate(all_threads[:5], 1):
            logger.info("\n  %d. %s", i, thread['title'])
            logger.info("     üìç URL: %s", thread['url'])
            logger.info("     üë§ ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô: %s", thread['author'])
            logger.info("     üëÅ  ‡∏¢‡∏≠‡∏î‡∏ß‡∏¥‡∏ß: %d", thread['views'])
            logger.info("     üí¨ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô: %d", thread['comments_count'])
            logger.info("     üìù ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: %d ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£", len(thread['content']))
            if thread['tags']:
                logger.info("     üè∑  Tags: %s", ', '.join(thread['tags'][:5]))
        
        if len(all_threads) > 5:
            logger.info("\n  ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ", len(all_threads) - 5)

        logger.info("\nüèÅ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        logger.info("%s", "=" * 80)
        
    except KeyboardInterrupt:
        logger.warning("\n\n‚ö†Ô∏è  ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
        logger.info("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ...", len(all_threads))
        
        try:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_threads, f, ensure_ascii=False, indent=2)
            logger.info("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: %s", OUTPUT_FILE)
        except Exception as save_error:
            logger.error("‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: %s", save_error)
        
    except Exception as e:
        logger.exception("\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: %s", e)
        import traceback
        traceback.print_exc()
        
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        if all_threads:
            logger.info("\nüíæ ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ...")
            try:
                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                    json.dump(all_threads, f, ensure_ascii=False, indent=2)
                logger.info("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: %d ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ", len(all_threads))
            except Exception as save_error:
                logger.error("‚ùå ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: %s", save_error)
    
    finally:
        # ‡∏õ‡∏¥‡∏î driver ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        try:
            driver.quit()
        except:
            pass  # Ignore errors when quitting driver


if __name__ == "__main__":
    setup_logging()
    main()
