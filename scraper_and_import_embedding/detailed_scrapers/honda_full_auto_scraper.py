"""
Honda Motorcycle Full Auto Scraper with Embeddings
‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å https://www.thaihonda.co.th/honda/motorcycle
- ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å div class="n_top"
- Specifications ‡∏à‡∏≤‡∏Å div class="n_name" ‡πÅ‡∏•‡∏∞ div class="value"
- ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏î‡πâ‡∏ß‡∏¢ Gemini API
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
"""
import os
import sys
import json
import time
import re
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Django setup
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'the_one.settings')
import django
django.setup()

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import google.generativeai as genai
from tqdm import tqdm

from chatbot.models import MotorcycleKnowledge

# Gemini API configuration
GEMINI_API_KEY = "AIzaSyDu2sGMNZPdAIhZUp0tsZ_7DrKDPqhwhtY"
genai.configure(api_key=GEMINI_API_KEY)

class HondaFullAutoScraper:
    def __init__(self):
        self.brand = "Honda"
        self.base_url = "https://www.thaihonda.co.th/honda/motorcycle"
        self.motorcycles = []
        self.driver = None
    
    def setup_driver(self, headless=False):
        """Setup Chrome WebDriver"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=chrome_options)
    
    def get_model_urls(self):
        """‡∏î‡∏∂‡∏á URL ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏ó‡∏∏‡∏Å‡∏£‡∏∏‡πà‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å"""
        print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ñ‡∏à‡∏≤‡∏Å {self.base_url}")
        self.driver = self.setup_driver(headless=False)
        
        try:
            self.driver.get(self.base_url)
            time.sleep(5)
            
            # Scroll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ‡∏î‡∏∂‡∏á link ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∏‡πà‡∏ô
            model_urls = []
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ links ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ products/motorcycles
            # ‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏£‡∏ñ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô section ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏ä‡πà‡∏ô "‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥", "‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏™‡πå‡πÉ‡∏´‡∏°‡πà", etc.
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href', '')
                # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ link ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ ‡πÄ‡∏ä‡πà‡∏ô /honda/motorcycle/sport/new-cbr250rr-2023
                if '/honda/motorcycle/' in href and href != '/honda/motorcycle':
                    full_url = f"https://www.thaihonda.co.th{href}" if not href.startswith('http') else href
                    if full_url not in model_urls:
                        model_urls.append(full_url)
            
            print(f"‚úÖ ‡∏û‡∏ö‡∏£‡∏ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(model_urls)} ‡∏£‡∏∏‡πà‡∏ô")
            return model_urls
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ñ: {e}")
            return []
    
    def extract_price_from_n_top(self, soup):
        """‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å top section (‡πÄ‡∏ä‡πà‡∏ô 'CBR250RR SP | start 269,000 THB')"""
        price_info = {}
        
        try:
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å h1, h2, h3 ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤
            headers = soup.find_all(['h1', 'h2', 'h3', 'div'], class_=re.compile(r'(title|top|price|model)', re.I))
            
            for header in headers:
                text = header.get_text(strip=True)
                
                # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö pattern "‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô | start xxx THB" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô | ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô xxx ‡∏ö‡∏≤‡∏ó"
                if '|' in text and ('start' in text.lower() or '‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô' in text or 'THB' in text or '‡∏ö‡∏≤‡∏ó' in text):
                    parts = text.split('|')
                    if len(parts) >= 2:
                        # ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô
                        price_info['model'] = parts[0].strip()
                        
                        # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤
                        price_part = parts[1].strip()
                        price_match = re.search(r'([\d,]+)', price_part)
                        if price_match:
                            price_num = price_match.group(1).replace(',', '')
                            price_info['price'] = {
                                'price': price_num,
                                'price_text': price_part,
                                'currency': 'THB'
                            }
                    break
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤‡πÅ‡∏¢‡∏Å - ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏≤‡∏Å h1/h2/h3, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å div/span
            if not price_info.get('model'):
                title_tags = soup.find_all(['h1', 'h2', 'h3'], limit=3)
                for tag in title_tags:
                    text = tag.get_text(strip=True)
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Ñ‡∏≤
                    if 5 < len(text) < 100 and not re.search(r'\d{3,}', text):
                        price_info['model'] = text
                        break
            
            if not price_info.get('price'):
                # ‡∏´‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤
                price_tags = soup.find_all(text=re.compile(r'(start|‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô|THB|‡∏ø|‡∏ö‡∏≤‡∏ó).*\d'))
                for tag in price_tags:
                    text = tag if isinstance(tag, str) else tag.get_text(strip=True)
                    price_match = re.search(r'([\d,]+)\s*(THB|‡∏ø|‡∏ö‡∏≤‡∏ó)', text)
                    if price_match:
                        price_num = price_match.group(1).replace(',', '')
                        price_info['price'] = {
                            'price': price_num,
                            'price_text': text.strip(),
                            'currency': 'THB'
                        }
                        break
        
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡πÑ‡∏î‡πâ: {e}")
        
        return price_info
    
    def extract_specifications(self, soup):
        """‡∏î‡∏∂‡∏á Specifications ‡∏à‡∏≤‡∏Å table/div structure"""
        specs = {}
        
        try:
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å table rows (‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
            # ‡∏´‡∏≤ table ‡∏ó‡∏µ‡πà‡∏°‡∏µ specifications
            spec_tables = soup.find_all('table') + soup.find_all('div', class_='n_container')
            
            for table in spec_tables:
                rows = table.find_all('tr') if table.name == 'table' else table.find_all('div', class_='n_desc')
                
                for row in rows:
                    # ‡∏´‡∏≤ cell ‡∏ã‡πâ‡∏≤‡∏¢ (‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå) ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ß‡∏≤ (‡∏Ñ‡πà‡∏≤)
                    if row.name == 'tr':
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            name = cells[0].get_text(strip=True)
                            value = cells[1].get_text(strip=True)
                            if name and value:
                                specs[name] = value
                    else:
                        # div structure
                        name_div = row.find('div', class_='n_name')
                        value_div = row.find('div', class_='value')
                        
                        if name_div and value_div:
                            name = name_div.get_text(strip=True)
                            value = value_div.get_text(strip=True)
                            if name and value:
                                specs[name] = value
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å dl/dt/dd
            if not specs:
                dts = soup.find_all('dt')
                dds = soup.find_all('dd')
                
                for dt, dd in zip(dts, dds):
                    name = dt.get_text(strip=True)
                    value = dd.get_text(strip=True)
                    if name and value:
                        specs[name] = value
            
            # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å div pairs ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
            if not specs:
                all_divs = soup.find_all('div')
                for i in range(len(all_divs) - 1):
                    div1 = all_divs[i]
                    div2 = all_divs[i + 1]
                    
                    # ‡∏ñ‡πâ‡∏≤ div ‡πÅ‡∏£‡∏Å‡∏°‡∏µ class ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô label/name
                    if 'label' in div1.get('class', []) or 'name' in div1.get('class', []):
                        if 'value' in div2.get('class', []) or 'data' in div2.get('class', []):
                            name = div1.get_text(strip=True)
                            value = div2.get_text(strip=True)
                            if name and value and len(name) < 100:  # ‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                                specs[name] = value
        
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• specifications ‡πÑ‡∏î‡πâ: {e}")
        
        return specs
    
    def scrape_model_page(self, url):
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ñ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∏‡πà‡∏ô"""
        try:
            print(f"\nüìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            # Scroll ‡∏•‡∏á‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î specifications
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # 1. ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å n_top
            price_info = self.extract_price_from_n_top(soup)
            
            # 2. ‡∏î‡∏∂‡∏á specifications
            specs = self.extract_specifications(soup)
            
            # 3. Extract model slug from URL
            model_slug = url.split('/')[-1]
            
            # 4. ‡∏î‡∏∂‡∏á features (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            features = []
            feature_sections = soup.find_all('div', class_='feature') or soup.find_all('div', class_='highlight')
            for section in feature_sections:
                feature_text = section.get_text(strip=True)
                if feature_text:
                    features.append(feature_text)
            
            # 5. ‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            images = []
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src', '')
                if src and ('motorcycle' in src or 'honda' in src):
                    images.append(src)
            
            # 6. ‡∏î‡∏∂‡∏á‡∏™‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            colors = []
            color_divs = soup.find_all('div', class_='color') or soup.find_all('div', class_='n_color')
            for color_div in color_divs:
                color_text = color_div.get_text(strip=True)
                if color_text:
                    colors.append(color_text)
            
            # 7. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö honda_bigbike_all.json
            motorcycle_data = {
                'brand': self.brand,
                'model': model_slug,
                'url': url,
                'category': 'Motorcycle',
                'scraped_at': datetime.now().isoformat(),
                **price_info,
                'specifications': specs,
                'features': features if features else [],
                'images': images if images else [],
                'colors': colors if colors else [],
                'description': ''
            }
            
            return motorcycle_data
            
        except Exception as e:
            print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {url}: {e}")
            return None
    
    def create_embedding(self, text):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏î‡πâ‡∏ß‡∏¢ Gemini API"""
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÑ‡∏î‡πâ: {e}")
            return None
    
    def prepare_text_for_embedding(self, motorcycle_data):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embedding"""
        text_parts = []
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏£‡∏∏‡πà‡∏ô
        text_parts.append(f"Brand: {motorcycle_data.get('brand', '')}")
        if 'model' in motorcycle_data:
            text_parts.append(f"Model: {motorcycle_data['model']}")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤
        if 'price' in motorcycle_data:
            text_parts.append(f"Price: {motorcycle_data['price']}")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° specifications
        if 'specifications' in motorcycle_data and motorcycle_data['specifications']:
            text_parts.append("Specifications:")
            for name, value in motorcycle_data['specifications'].items():
                text_parts.append(f"{name}: {value}")
        
        return "\n".join(text_parts)
    
    def scrape_all_models(self, create_embeddings=False):
        """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏£‡∏ñ‡∏ó‡∏∏‡∏Å‡∏£‡∏∏‡πà‡∏ô"""
        # 1. ‡∏î‡∏∂‡∏á URLs ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        model_urls = self.get_model_urls()
        
        if not model_urls:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö URL ‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ")
            return
        
        # 2. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏ñ
        print(f"\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {len(model_urls)} ‡∏£‡∏∏‡πà‡∏ô...")
        if create_embeddings:
            print("ü§ñ ‡πÇ‡∏´‡∏°‡∏î: ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• + ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings")
        else:
            print("‚ö†Ô∏è ‡πÇ‡∏´‡∏°‡∏î: ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings)")
        
        for idx, url in enumerate(tqdm(model_urls, desc="üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"), 1):
            motorcycle_data = self.scrape_model_page(url)
            
            if motorcycle_data:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
                if create_embeddings:
                    text = self.prepare_text_for_embedding(motorcycle_data)
                    embedding = self.create_embedding(text)
                    
                    if embedding:
                        motorcycle_data['embedding'] = embedding
                        motorcycle_data['embedding_dimension'] = len(embedding)
                
                self.motorcycles.append(motorcycle_data)
                print(f"‚úÖ [{idx}/{len(model_urls)}] {motorcycle_data.get('model', 'Unknown')} - ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
            
            # Delay ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å
            time.sleep(2)
        
        # 3. ‡∏õ‡∏¥‡∏î browser
        if self.driver:
            self.driver.quit()
    
    def save_to_json(self, filename='honda_motorcycles_full.json'):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON file"""
        output_path = Path(__file__).parent / filename
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.motorcycles, f, ensure_ascii=False, indent=2)
            
            print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {output_path}")
            print(f"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(self.motorcycles)} ‡∏£‡∏∏‡πà‡∏ô")
            
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: {e}")
    
    def save_to_database(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Django database"""
        print("\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        saved_count = 0
        
        for motorcycle in self.motorcycles:
            try:
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                text = self.prepare_text_for_embedding(motorcycle)
                embedding = motorcycle.get('embedding')
                
                if not embedding:
                    print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {motorcycle.get('model')} - ‡πÑ‡∏°‡πà‡∏°‡∏µ embedding")
                    continue
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á database
                obj, created = MotorcycleKnowledge.objects.update_or_create(
                    source='honda_website',
                    brand=motorcycle.get('brand', 'Honda'),
                    model=motorcycle.get('model', 'Unknown'),
                    defaults={
                        'category': 'specifications',
                        'title': motorcycle.get('model', 'Unknown'),
                        'content': text,
                        'price': motorcycle.get('price', ''),
                        'specifications': json.dumps(motorcycle.get('specifications', {}), ensure_ascii=False),
                        'url': motorcycle.get('url', ''),
                        'embedding': embedding
                    }
                )
                
                saved_count += 1
                status = "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà" if created else "‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó"
                print(f"‚úÖ {status}: {motorcycle.get('model')} ")
                
            except Exception as e:
                print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {motorcycle.get('model')} ‡πÑ‡∏î‡πâ: {e}")
        
        print(f"\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {saved_count}/{len(self.motorcycles)} ‡∏£‡∏∏‡πà‡∏ô")


def main():
    """Main function"""
    print("=" * 80)
    print("üèçÔ∏è  Honda Motorcycle Full Auto Scraper")
    print("=" * 80)
    
    # ‡∏ñ‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    create_emb = input("\nü§ñ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏î‡πâ‡∏ß‡∏¢ Gemini API ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): ")
    create_embeddings = create_emb.lower() == 'y'
    
    scraper = HondaFullAutoScraper()
    
    # 1. ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    scraper.scrape_all_models(create_embeddings=create_embeddings)
    
    # 2. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON
    scraper.save_to_json()
    
    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ embeddings)
    if create_embeddings:
        save_db = input("\nüíæ ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà? (y/n): ")
        if save_db.lower() == 'y':
            scraper.save_to_database()
    else:
        print("\n‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ embeddings")
    
    print("\n" + "=" * 80)
    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!")
    print("=" * 80)


if __name__ == "__main__":
    main()
