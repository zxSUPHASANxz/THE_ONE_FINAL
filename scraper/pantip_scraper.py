#!/usr/bin/env python3
"""
Pantip Motorcycle Scraper - Auto Search & Extract
‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Pantip ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
"""

import json
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from datetime import datetime, timedelta


def create_chrome_driver():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Chrome WebDriver ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver


def clean_text(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def search_pantip(driver, keywords, max_retries=3):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡πÉ‡∏ô Pantip ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"""
    print(f"\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {keywords}")
    
    search_url = f"https://pantip.com/search?q={keywords}"
    
    # Retry logic with exponential backoff
    for attempt in range(max_retries):
        try:
            driver.get(search_url)
            time.sleep(3)
            break
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt
                print(f"  ‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} (wait {wait_time}s)...")
                time.sleep(wait_time)
            else:
                raise e
    
    # Scroll ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # ‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    thread_links = []
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å div.post-item
    posts = soup.find_all('div', class_='post-item')
    for post in posts:
        link = post.find('a', href=True)
        if link and '/topic/' in link.get('href'):
            full_url = f"https://pantip.com{link.get('href')}" if link.get('href').startswith('/') else link.get('href')
            if full_url not in thread_links:
                thread_links.append(full_url)
    
    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏´‡∏≤‡∏à‡∏≤‡∏Å a tag ‡∏ó‡∏µ‡πà‡∏°‡∏µ /topic/
    if len(thread_links) == 0:
        links = soup.find_all('a', href=re.compile(r'/topic/\d+'))
        for link in links:
            full_url = f"https://pantip.com{link.get('href')}" if link.get('href').startswith('/') else link.get('href')
            # ‡∏ï‡∏±‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏≠‡∏Å
            full_url = full_url.split('?')[0]
            if full_url not in thread_links:
                thread_links.append(full_url)
    
    print(f"‚úÖ ‡∏û‡∏ö {len(thread_links)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ")
    return thread_links


def scrape_thread_content(driver, url, max_retries=2):
    """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ Pantip"""
    for attempt in range(max_retries):
        try:
            print(f"  üåê ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: {url}")
            driver.get(url)
            
            # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ WebDriverWait - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "article"))
                )
            except:
                pass
            
            # ‡∏£‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö lazy loading
            time.sleep(5)
            
            # Scroll ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î comments ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            last_height = driver.execute_script("return document.body.scrollHeight")
            for i in range(5):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
            
            # Scroll ‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # Scroll ‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            thread_data = {
                'url': url,
                'title': '',
                'author': '',
                'date': '',
                'views': 0,
                'comments_count': 0,
                'tags': [],
                'content': '',
                'comments': []
            }
            
            # Title - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            title_tag = soup.find('h1', class_='display-post-title')
            if not title_tag:
                title_tag = soup.find('h1', class_=re.compile(r'title'))
            if not title_tag:
                title_tag = soup.find('h1')
            if title_tag:
                thread_data['title'] = clean_text(title_tag.get_text())
                print(f"  üìÑ {thread_data['title'][:80]}")
            
            # Author - ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            author_tag = soup.find('a', class_='owner')
            if not author_tag:
                author_tag = soup.find('div', class_='display-post-name')
            if not author_tag:
                author_tag = soup.find('a', class_=re.compile(r'author'))
            if author_tag:
                thread_data['author'] = clean_text(author_tag.get_text())
            
            # Date
            date_tag = soup.find('abbr', class_='timeago')
            if not date_tag:
                date_tag = soup.find('time')
            if date_tag:
                thread_data['date'] = date_tag.get('title', '') or date_tag.get('datetime', '')
            
            # Views
            views_tag = soup.find('span', class_='view-count')
            if views_tag:
                views_text = clean_text(views_tag.get_text())
                views_match = re.search(r'([\d,]+)', views_text)
                if views_match:
                    thread_data['views'] = int(views_match.group(1).replace(',', ''))
            
            # Comments count
            comment_count_tag = soup.find('span', class_='comments-count')
            if comment_count_tag:
                count_text = clean_text(comment_count_tag.get_text())
                count_match = re.search(r'(\d+)', count_text)
                if count_match:
                    thread_data['comments_count'] = int(count_match.group(1))
            
            # Tags
            tag_links = soup.find_all('a', class_='tag-item')
            if not tag_links:
                tag_links = soup.find_all('a', href=re.compile(r'/tag/'))
            thread_data['tags'] = [clean_text(tag.get_text()) for tag in tag_links if tag.get_text().strip()]
            
            # ===== ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å =====
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            content_div = soup.find('div', class_='display-post-story')
            if not content_div:
                content_div = soup.find('div', class_=re.compile(r'post-story'))
            if not content_div:
                content_div = soup.find('div', attrs={'data-type': 'message'})
            if not content_div:
                # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å article ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏≤ div ‡πÅ‡∏£‡∏Å
                article = soup.find('article')
                if article:
                    content_div = article.find('div', class_=re.compile(r'(content|story|message|body)'))
            
            if content_div:
                # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                for unwanted in content_div.find_all(['script', 'style', 'iframe']):
                    unwanted.decompose()
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° - ‡πÉ‡∏ä‡πâ get_text() ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
                content_text = content_div.get_text(separator='\n', strip=True)
                thread_data['content'] = clean_text(content_text)
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å paragraph ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            if not thread_data['content'] or len(thread_data['content']) < 20:
                # ‡∏´‡∏≤‡∏ó‡∏∏‡∏Å p tag ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô article ‡∏´‡∏£‡∏∑‡∏≠ main content
                main_content = soup.find('article') or soup.find('main') or soup
                paragraphs = []
                for p in main_content.find_all(['p', 'div'], class_=re.compile(r'(paragraph|text-display)')):
                    text = clean_text(p.get_text())
                    if text and len(text) > 15 and 'comment' not in p.get('class', []):
                        paragraphs.append(text)
                
                if paragraphs:
                    thread_data['content'] = '\n\n'.join(paragraphs[:10])  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 paragraphs ‡πÅ‡∏£‡∏Å
            
            # ===== ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå =====
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ
            comment_divs = soup.find_all('div', class_='comment-item')
            if not comment_divs:
                comment_divs = soup.find_all('div', class_=re.compile(r'comment-wrapper'))
            if not comment_divs:
                comment_divs = soup.find_all('article', class_=re.compile(r'comment'))
            if not comment_divs:
                # ‡∏´‡∏≤‡∏à‡∏≤‡∏Å div ‡∏ó‡∏µ‡πà‡∏°‡∏µ id ‡∏´‡∏£‡∏∑‡∏≠ class ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö comment
                comment_section = soup.find('div', id=re.compile(r'comment', re.I))
                if comment_section:
                    comment_divs = comment_section.find_all('div', recursive=True)
            
            comment_count = 0
            for comment_div in comment_divs:
                if comment_count >= 30:  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 30 comments
                    break
                
                # ‡∏´‡∏≤ author
                comment_author_tag = comment_div.find('a', class_=re.compile(r'(author|name|owner)'))
                if not comment_author_tag:
                    comment_author_tag = comment_div.find('span', class_=re.compile(r'(author|name|user)'))
                if not comment_author_tag:
                    comment_author_tag = comment_div.find('div', class_=re.compile(r'name'))
                
                # ‡∏´‡∏≤ content
                comment_text_tag = comment_div.find('div', class_=re.compile(r'(story|content|text|message|body)'))
                if not comment_text_tag:
                    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å p tags
                    comment_paragraphs = comment_div.find_all('p')
                    if comment_paragraphs:
                        comment_text_tag = comment_paragraphs[0].parent
                
                if comment_text_tag:
                    # ‡∏•‡∏ö elements ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                    for unwanted in comment_text_tag.find_all(['script', 'style', 'iframe']):
                        unwanted.decompose()
                    
                    comment_text = clean_text(comment_text_tag.get_text(separator='\n', strip=True))
                    
                    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏¢‡∏≤‡∏ß‡∏û‡∏≠ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà metadata
                    if len(comment_text) > 20 and not re.match(r'^(Like|Reply|Report|Share|\d+)', comment_text):
                        comment_count += 1
                        thread_data['comments'].append({
                            'author': clean_text(comment_author_tag.get_text()) if comment_author_tag else 'Anonymous',
                            'content': comment_text
                        })
            
            print(f"  ‚úÖ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {len(thread_data['content'])} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£, {len(thread_data['comments'])} comments")
            
            return thread_data
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"  ‚ö†Ô∏è  Retry {attempt + 1}/{max_retries}...")
                time.sleep(2)
                continue
            else:
                print(f"  ‚ùå Error: {e}")
                return None
    
    return None


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    
    # ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 110+ ‡∏Ñ‡∏≥
    SEARCH_KEYWORDS = [
        # ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
        "bigbike ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
        "bigbike ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
        "bigbike ‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà",
        "bigbike ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ñ‡∏±‡∏ô‡πÅ‡∏£‡∏Å",
        "bigbike ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å",
        
        # ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 250cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 300cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 400cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 500cc",
        "‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 650cc",
        
        # Kawasaki
        "ninja 250",
        "ninja 300",
        "ninja 400",
        "ninja 650",
        "ninja 1000",
        "z250",
        "z300",
        "z400",
        "z650",
        "z900",
        "versys 650",
        "versys 1000",
        "er6n",
        "er6f",
        
        # Honda
        "cbr150",
        "cbr250",
        "cbr300",
        "cbr500",
        "cbr650",
        "cbr1000",
        "cb150r",
        "cb300f",
        "cb500x",
        "cb650r",
        "rebel 300",
        "rebel 500",
        "pcx 150",
        "pcx 160",
        "forza 300",
        
        # Yamaha
        "r15",
        "r3",
        "r6",
        "r1",
        "mt-03",
        "mt-07",
        "mt-09",
        "mt-10",
        "xsr700",
        "xsr900",
        "tracer 700",
        "tracer 900",
        "aerox 155",
        "nmax 155",
        
        # Suzuki
        "gsx-r150",
        "gsx-s150",
        "gsx250r",
        "gsx-s750",
        "gsx-s1000",
        "sv650",
        "v-strom 650",
        "hayabusa",
        
        # KTM
        "duke 200",
        "duke 250",
        "duke 390",
        "duke 790",
        "rc 200",
        "rc 390",
        "adventure 390",
        
        # BMW
        "bmw s1000rr",
        "bmw f750gs",
        "bmw f850gs",
        "bmw r1250gs",
        
        # Ducati
        "ducati monster",
        "ducati panigale",
        "ducati scrambler",
        
        # Triumph
        "triumph street triple",
        "triumph speed triple",
        "triumph tiger",
        
        # Harley Davidson
        "harley davidson",
        "harley sportster",
        
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏ã‡πà‡∏≠‡∏°‡∏°‡∏≠‡πÑ‡∏ã",
        "‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≠‡πÑ‡∏ã",
        "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏°‡∏≠‡πÑ‡∏ã",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏Å‡∏©‡∏≤ ===
        "‡∏ã‡πà‡∏≠‡∏°‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ñ‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏•‡πâ‡∏≤‡∏á‡πÇ‡∏ã‡πà‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏î‡∏π‡πÅ‡∏•‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà/‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå ===
        "‡∏¢‡∏≤‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÑ‡∏ü‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ó‡πà‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ú‡πâ‡∏≤‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÇ‡∏ä‡πâ‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏Ñ‡∏•‡∏±‡∏ï‡∏ä‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤/‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏î‡∏±‡∏ö",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏™‡∏ï‡∏≤‡∏£‡πå‡∏ó‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î",
        "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏£‡πâ‡∏≠‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏™‡∏±‡πà‡∏ô",
        "‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏¢‡∏≤‡∏Å",
        
        # === ‡∏£‡∏∏‡πà‡∏ô‡∏£‡∏ñ‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ===
        "xmax 300",
        "tmax 560",
        "msx 125",
        "monkey 125",
        "grom",
        "benelli tnt 150",
        "benelli 302r",
        "cfmoto 300nk",
        "gpx gentleman 200",
        "gpx legend 250",
        
        # === ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ó‡∏±‡∏ß‡∏£‡πå‡∏£‡∏¥‡πà‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÑ‡∏Å‡∏•‡πÅ‡∏î‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏î‡∏≠‡∏¢",
        "‡∏Ç‡∏±‡∏ö‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ñ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏Ñ‡∏ô‡πÄ‡∏ï‡∏µ‡πâ‡∏¢",
        
        # === ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢/‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô ===
        "‡∏Ñ‡πà‡∏≤‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏†‡∏≤‡∏©‡∏µ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ã‡∏∑‡πâ‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ú‡πà‡∏≠‡∏ô",
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏°‡∏∑‡∏≠‡∏™‡∏≠‡∏á",
        
        # === ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå Safety ===
        "‡∏´‡∏°‡∏ß‡∏Å‡∏Å‡∏±‡∏ô‡∏ô‡πá‡∏≠‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏ñ‡∏∏‡∏á‡∏°‡∏∑‡∏≠‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "‡∏£‡∏≠‡∏á‡πÄ‡∏ó‡πâ‡∏≤‡∏ö‡∏π‡πä‡∏ó‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        "ABS ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå",
        
        # === ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ===
        "‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô‡∏£‡∏±‡πà‡∏ß",
        "‡∏¢‡∏≤‡∏á‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÅ‡∏ï‡∏Å",
        "‡∏Ñ‡∏•‡∏±‡∏ï‡∏ä‡πå‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏•‡∏∑‡πà‡∏ô",
        "‡πÄ‡∏ö‡∏£‡∏Ñ‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏≠‡πà‡∏≠‡∏ô",
        "‡πÑ‡∏ü‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î",
        "‡πÅ‡∏ö‡∏ï‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏´‡∏°‡∏î",
    ]
    
    OUTPUT_FILE = "pantip.json"
    MAX_THREADS_PER_KEYWORD = 15  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 15 ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô
    RECREATE_DRIVER_EVERY = 10  # ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å 10 keywords
    
    print("=" * 80)
    print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Pantip Motorcycle Scraper - Enhanced Version (Error-Resistant)")
    print("=" * 80)
    print(f"üìã ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {len(SEARCH_KEYWORDS)} ‡∏Ñ‡∏≥")
    print(f"üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏°‡∏≠‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏ã‡∏î‡πå 150cc+ ‡πÅ‡∏•‡∏∞‡∏ö‡∏¥‡πä‡∏Å‡πÑ‡∏ö‡∏Ñ‡πå‡∏ó‡∏∏‡∏Å‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠")
    print(f"üìÅ ‡πÑ‡∏ü‡∏•‡πå output: {OUTPUT_FILE}")
    print(f"üî¢ ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î {MAX_THREADS_PER_KEYWORD} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ/‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô")
    print(f"üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å {RECREATE_DRIVER_EVERY} keywords (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô memory leak)")
    print("=" * 80)
    
    driver = create_chrome_driver()
    
    all_threads = []
    seen_urls = set()
    start_time = datetime.now()
    
    try:
        for idx, keyword in enumerate(SEARCH_KEYWORDS):
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì progress
            progress_pct = ((idx) / len(SEARCH_KEYWORDS)) * 100
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ETA
            if idx > 0:
                elapsed = (datetime.now() - start_time).total_seconds()
                avg_time_per_keyword = elapsed / idx
                remaining_keywords = len(SEARCH_KEYWORDS) - idx
                eta_seconds = avg_time_per_keyword * remaining_keywords
                eta_time = datetime.now() + timedelta(seconds=eta_seconds)
                
                print(f"\n‚è±Ô∏è  Progress: {idx}/{len(SEARCH_KEYWORDS)} ({progress_pct:.1f}%) | ETA: {eta_time.strftime('%H:%M:%S')}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å N keywords ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô session expire
            if idx > 0 and idx % RECREATE_DRIVER_EVERY == 0:
                print(f"üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß {idx}/{len(SEARCH_KEYWORDS)} keywords)...")
                try:
                    driver.quit()
                except:
                    pass
                driver = create_chrome_driver()
                print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\n")
                time.sleep(3)
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ
            try:
                thread_urls = search_pantip(driver, keyword)
            except Exception as e:
                print(f"‚ùå Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ '{keyword}': {e}")
                print("üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á...")
                try:
                    driver.quit()
                except:
                    pass
                driver = create_chrome_driver()
                time.sleep(3)
                try:
                    thread_urls = search_pantip(driver, keyword)
                except Exception as e2:
                    print(f"‚ùå ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á error: {e2}")
                    print("‚è≠Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏ô‡∏µ‡πâ\n")
                    continue
            
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
            thread_urls = thread_urls[:MAX_THREADS_PER_KEYWORD]
            
            # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ
            print(f"\nüìù ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {len(thread_urls)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ...\n")
            
            for i, url in enumerate(thread_urls, 1):
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ã‡πâ‡∏≥
                if url in seen_urls:
                    print(f"  [{i}/{len(thread_urls)}] ‚è≠Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° (‡∏ã‡πâ‡∏≥): {url}")
                    continue
                
                seen_urls.add(url)
                print(f"  [{i}/{len(thread_urls)}]", end=" ")
                
                # ‡∏•‡∏≠‡∏á‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏ñ‡πâ‡∏≤ error ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà
                thread_data = None
                try:
                    thread_data = scrape_thread_content(driver, url)
                except Exception as e:
                    print(f"  ‚ùå Error: {e}")
                    print("  üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á driver ‡πÉ‡∏´‡∏°‡πà...")
                    try:
                        driver.quit()
                    except:
                        pass
                    driver = create_chrome_driver()
                    time.sleep(2)
                    # ‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                    try:
                        thread_data = scrape_thread_content(driver, url)
                    except:
                        print("  ‚è≠Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ô‡∏µ‡πâ")
                
                if thread_data and thread_data.get('content'):
                    all_threads.append(thread_data)
                
                # ‡∏û‡∏±‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                time.sleep(2)
            
            print(f"\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: {keyword} ({len(thread_urls)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ)")
            print(f"üìä ‡∏£‡∏ß‡∏°‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ: {len(all_threads)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ | ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥: {len(seen_urls)} URLs")
            
            # ‡πÅ‡∏™‡∏î‡∏á progress bar
            completed = idx + 1
            progress_pct = (completed / len(SEARCH_KEYWORDS)) * 100
            bar_length = 40
            filled = int(bar_length * completed / len(SEARCH_KEYWORDS))
            bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
            print(f"üìà [{bar}] {progress_pct:.1f}%")
            print("-" * 80)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        print("\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_threads, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 80)
        print(f"‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå: {OUTPUT_FILE}")
        print(f"üìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_threads)}")
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        import os
        file_size = os.path.getsize(OUTPUT_FILE)
        print(f"üìÅ ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå: {file_size:,} bytes ({file_size/1024:.2f} KB)")
        
        total_content = sum(len(t['content']) for t in all_threads)
        total_comments = sum(len(t['comments']) for t in all_threads)
        total_views = sum(t['views'] for t in all_threads)
        
        print(f"\nüìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:")
        print(f"  - ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏£‡∏ß‡∏°: {total_content:,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        print(f"  - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏ß‡∏°: {total_comments:,} comments")
        print(f"  - ‡∏¢‡∏≠‡∏î‡∏ß‡∏¥‡∏ß‡∏£‡∏ß‡∏°: {total_views:,} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        print(f"  - ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: {total_content//len(all_threads) if all_threads else 0:,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
        print(f"  - comments ‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ: {total_comments//len(all_threads) if all_threads else 0:.1f}")
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏û‡∏ö
        brands_found = {}
        brand_keywords = ['honda', 'yamaha', 'kawasaki', 'suzuki', 'ktm', 'ducati', 'bmw', 'triumph', 'harley']
        for thread in all_threads:
            title_lower = thread['title'].lower()
            for brand in brand_keywords:
                if brand in title_lower:
                    brands_found[brand] = brands_found.get(brand, 0) + 1
        
        if brands_found:
            print(f"\nüèçÔ∏è  ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:")
            for brand, count in sorted(brands_found.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  - {brand.upper()}: {count} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        print(f"\nüìã ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ (‡πÅ‡∏™‡∏î‡∏á 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å):")
        for i, thread in enumerate(all_threads[:5], 1):
            print(f"\n  {i}. {thread['title']}")
            print(f"     üìç URL: {thread['url']}")
            print(f"     üë§ ‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô: {thread['author']}")
            print(f"     üëÅ  ‡∏¢‡∏≠‡∏î‡∏ß‡∏¥‡∏ß: {thread['views']:,}")
            print(f"     üí¨ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô: {thread['comments_count']:,}")
            print(f"     üìù ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {len(thread['content']):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£")
            if thread['tags']:
                print(f"     üè∑  Tags: {', '.join(thread['tags'][:5])}")
        
        if len(all_threads) > 5:
            print(f"\n  ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(all_threads) - 5} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ")
        
        print("\nüèÅ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß {len(all_threads)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ...")
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_threads, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: {OUTPUT_FILE}")
        
    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback
        traceback.print_exc()
        
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
        if all_threads:
            print(f"\nüíæ ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ...")
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(all_threads, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: {len(all_threads)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ")
    
    finally:
        driver.quit()


if __name__ == "__main__":
    main()
